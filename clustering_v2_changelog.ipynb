{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Analysis v2 — Changelog\n",
    "\n",
    "This notebook documents every change made in `clustering_analysis_v2.ipynb` relative to the baseline `clustering_analysis.ipynb`. The improvements fall into three categories: **reproducibility**, **methodological rigour**, and **code quality**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Changes\n",
    "\n",
    "| # | Area | Baseline | v2 | Cells affected |\n",
    "|---|------|----------|----|----------------|\n",
    "| 1 | Reproducibility | No random seeds set | Global `SEED = 42` with `random.seed`, `np.random.seed` | cell-3 (imports) |\n",
    "| 2 | Unused import | `from collections import Counter` imported but never used | Removed | cell-3 (imports) |\n",
    "| 3 | UMAP performance | Two independent UMAP fits, each computing its own NN graph | NN graph computed once via `nearest_neighbors()`, shared via `precomputed_knn` | cell-11, cell-12 |\n",
    "| 4 | Plot code | Inline scatter logic duplicated across 5+ cells | Reusable `umap_scatter()` helper function | cell-14, cell-15, cell-20, cell-21, cell-30 |\n",
    "| 5 | HDBSCAN sweep | 1D sweep over `min_cluster_size` only (5 values) | 3D sweep over `min_cluster_size` x `min_samples` x `epsilon` (64 configs) | cell-17 |\n",
    "| 6 | Selection metric | Best config chosen by **ARI** (uses ground-truth labels — circular) | Best config chosen by **DBCV** (internal, unsupervised) | cell-17, cell-18 |\n",
    "| 7 | Cluster count | No constraint on number of clusters | Filtered to **<= 30 clusters** before DBCV selection | cell-17, cell-18 |\n",
    "| 8 | DBCV computation | Not computed (`gen_min_span_tree` not enabled) | `gen_min_span_tree=True` on all HDBSCAN runs | cell-17, cell-18 |\n",
    "| 9 | Noise handling | Noise points excluded from all evaluation | `approximate_predict` assigns noise to nearest cluster; both excl-noise and full-dataset metrics reported | cell-23 |\n",
    "| 10 | Centroid distance | Euclidean distance for representative sample selection | **Cosine distance** (consistent with UMAP metric) | cell-32 |\n",
    "| 11 | LLM API calls | Sequential (one-at-a-time) | **Concurrent** via `AsyncAnthropic` + `asyncio.gather` | cell-32 |"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Background: What Is DBCV?\n\n**DBCV** (Density-Based Clustering Validation) is an **internal** cluster validity index designed specifically for density-based clustering algorithms like HDBSCAN. It was introduced by Moulavi et al. (2014) and is available in the HDBSCAN library as `clusterer.relative_validity_`.\n\n### The Core Idea\n\nTraditional internal metrics like the Silhouette Score assume clusters are convex, globular shapes — they measure how close each point is to the center of its own cluster versus the center of the nearest other cluster. This assumption breaks down for density-based algorithms, which can discover clusters of arbitrary shape.\n\nDBCV instead evaluates clustering quality in terms of **density**:\n\n1. **Intra-cluster density** — For each cluster, DBCV builds a *minimum spanning tree* (MST) over its member points using a **mutual reachability distance** (a density-aware distance that inflates distances in sparse regions). The *densest* internal connection within the cluster defines how tightly packed it is.\n\n2. **Inter-cluster density** — For each pair of clusters, DBCV finds the densest path *between* them. If clusters are well-separated, this inter-cluster density will be low.\n\n3. **Per-cluster score** — For each cluster, DBCV computes:\n\n$$\\text{DBCV}_i = \\frac{\\text{min inter-cluster density} - \\text{max intra-cluster density}}{\\max(\\text{min inter-cluster density},\\; \\text{max intra-cluster density})}$$\n\nThe value ranges from **-1 to +1**: positive means the cluster is denser internally than the space separating it from other clusters; negative means clusters are overlapping.\n\n4. **Global score** — The overall DBCV is the density-weighted average of per-cluster scores:\n\n$$\\text{DBCV} = \\frac{\\sum_i n_i \\cdot \\text{DBCV}_i}{\\sum_i n_i}$$\n\nwhere $n_i$ is the number of points in cluster $i$.\n\n### Why Is DBCV Needed?\n\n| Problem | How DBCV solves it |\n|---------|--------------------|\n| **Circular evaluation** — The baseline selects HDBSCAN parameters by maximising ARI against ground-truth labels. This is cheating: in a real application those labels don't exist. | DBCV is purely **internal** — it evaluates the clustering using only the data geometry, with no reference to external labels. |\n| **Shape assumptions** — Silhouette Score and other common metrics assume convex, roughly spherical clusters. HDBSCAN intentionally discovers clusters of arbitrary shape. | DBCV is designed for **density-based** clusterings and correctly handles non-convex, irregularly shaped clusters. |\n| **Noise awareness** — Standard metrics have no notion of noise points. | DBCV naturally integrates with HDBSCAN's noise model — only clustered points contribute to the score. |\n\n### Interpretation\n\n| DBCV Range | Meaning |\n|------------|---------|\n| **0.75 – 1.0** | Excellent — clusters are dense and well-separated |\n| **0.50 – 0.75** | Good — clear density structure |\n| **0.25 – 0.50** | Moderate — some cluster overlap |\n| **0.00 – 0.25** | Weak — clusters barely distinguishable from surrounding space |\n| **< 0.00** | Poor — clusters overlap significantly |\n\n### Practical Note\n\nDBCV requires the minimum spanning tree to be computed, which is why v2 sets `gen_min_span_tree=True` on every HDBSCAN call. Without this flag, accessing `clusterer.relative_validity_` raises an `AttributeError`.\n\n### Reference\n\nMoulavi, D., Jaskowiak, P.A., Campello, R.J.G.B., Zimek, A., & Sander, J. (2014). *Density-Based Clustering Validation*. In Proceedings of the 2014 SIAM International Conference on Data Mining (SDM), pp. 839–847.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Reproducibility — Random Seeds\n",
    "\n",
    "**Problem:** The baseline sets `random_state=42` on individual UMAP objects but does not seed the global Python or NumPy random number generators. Any code path that draws from these (e.g. random sampling, pair generation) may produce different results across runs.\n",
    "\n",
    "**Fix:** v2 adds a global `SEED` constant and seeds all relevant generators at the top of the notebook.\n",
    "\n",
    "```python\n",
    "# ── Baseline (cell-3) ──────────────────────────────────\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# ... no seeds set ...\n",
    "\n",
    "# ── v2 (cell-3) ───────────────────────────────────────\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Unused Import Removed\n",
    "\n",
    "**Problem:** The baseline imports `from collections import Counter` but never uses it anywhere in the notebook.\n",
    "\n",
    "**Fix:** v2 removes the import.\n",
    "\n",
    "```python\n",
    "# ── Baseline (cell-3) ──────────────────────────────────\n",
    "from collections import Counter  # never used\n",
    "\n",
    "# ── v2 (cell-3) ───────────────────────────────────────\n",
    "# removed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. UMAP — Precomputed Nearest-Neighbor Graph\n",
    "\n",
    "**Problem:** The baseline creates two separate UMAP objects (15d for clustering, 2d for visualization) in separate cells. Each independently computes a nearest-neighbor graph over the same 26,872-point, 384-dimensional embedding matrix with the same `n_neighbors=15` and `metric=\"cosine\"`. The NN search is the most expensive step in UMAP, so this work is done twice.\n",
    "\n",
    "**Fix:** v2 computes the NN graph once using `umap.umap_.nearest_neighbors()` and passes the result tuple `(knn_indices, knn_dists, knn_forest)` to both UMAP instances via the `precomputed_knn` parameter.\n",
    "\n",
    "```python\n",
    "# ── Baseline (cell-11 and cell-12) ─────────────────────\n",
    "# Cell 11: UMAP clustering — builds its own NN graph\n",
    "umap_clustering = umap.UMAP(\n",
    "    n_components=15, n_neighbors=15, min_dist=0.0,\n",
    "    metric=\"cosine\", random_state=42, verbose=True\n",
    ")\n",
    "embeddings_umap_cluster = umap_clustering.fit_transform(embeddings)\n",
    "\n",
    "# Cell 12: UMAP viz — builds its own NN graph (again)\n",
    "umap_viz = umap.UMAP(\n",
    "    n_components=2, n_neighbors=15, min_dist=0.1,\n",
    "    metric=\"cosine\", random_state=42, verbose=True\n",
    ")\n",
    "embeddings_2d = umap_viz.fit_transform(embeddings)\n",
    "\n",
    "# ── v2 (cell-11 and cell-12) ──────────────────────────\n",
    "# Cell 11: compute NN graph once\n",
    "from umap.umap_ import nearest_neighbors\n",
    "knn_indices, knn_dists, knn_forest = nearest_neighbors(\n",
    "    embeddings, n_neighbors=15, metric=\"cosine\", metric_kwds={},\n",
    "    angular=True, random_state=np.random.RandomState(SEED), verbose=True\n",
    ")\n",
    "precomputed_knn = (knn_indices, knn_dists, knn_forest)\n",
    "\n",
    "# Cell 12: both UMAP projections reuse the precomputed NN graph\n",
    "umap_clustering = umap.UMAP(\n",
    "    ..., precomputed_knn=precomputed_knn\n",
    ")\n",
    "umap_viz = umap.UMAP(\n",
    "    ..., precomputed_knn=precomputed_knn\n",
    ")\n",
    "```\n",
    "\n",
    "**Impact:** Eliminates one full NN search (~10-15 seconds on this dataset), roughly halving the UMAP wall time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reusable Scatter Plot Helper\n",
    "\n",
    "**Problem:** The baseline has nearly identical scatter-plotting code duplicated across multiple cells (ground-truth category, ground-truth intent, HDBSCAN clusters, side-by-side comparison, annotated plot). Each copy handles noise, palettes, and axis labels inline.\n",
    "\n",
    "**Fix:** v2 extracts a `umap_scatter()` function that encapsulates the common logic:\n",
    "\n",
    "```python\n",
    "def umap_scatter(df, color_col, ax, palette, noise_labels=None, s=3, alpha=0.3):\n",
    "    \"\"\"Reusable UMAP scatter plot colored by a categorical column.\"\"\"\n",
    "    labels_sorted = sorted(df[color_col].unique())\n",
    "    if noise_labels is not None:\n",
    "        noise_mask = df[color_col].isin(...)\n",
    "        if noise_mask.any():\n",
    "            ax.scatter(..., c=\"lightgrey\", s=1, alpha=0.2, label=f\"Noise (...)\")\n",
    "        labels_sorted = [l for l in labels_sorted if l not in ...]\n",
    "    for i, label in enumerate(labels_sorted):\n",
    "        mask = df[color_col] == label\n",
    "        ax.scatter(...)\n",
    "    ax.set_xlabel(\"UMAP-1\")\n",
    "    ax.set_ylabel(\"UMAP-2\")\n",
    "```\n",
    "\n",
    "All 5 scatter plot cells now call `umap_scatter()` with different parameters, eliminating ~60 lines of duplicated code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Expanded HDBSCAN Parameter Sweep\n",
    "\n",
    "**Problem:** The baseline sweeps only over `min_cluster_size` in `[25, 50, 100, 200, 500]` with a fixed `min_samples=10` and no `cluster_selection_epsilon`. This is a 1D sweep with just 5 configurations.\n",
    "\n",
    "**Fix:** v2 performs a 3D sweep over three parameters:\n",
    "\n",
    "| Parameter | Baseline | v2 |\n",
    "|-----------|----------|----|\n",
    "| `min_cluster_size` | `[25, 50, 100, 200, 500]` | `[200, 500, 750, 1000]` |\n",
    "| `min_samples` | fixed at `10` | `[5, 10, 25, 50]` |\n",
    "| `cluster_selection_epsilon` | not used (`0.0`) | `[0.0, 0.5, 1.0, 2.0]` |\n",
    "| **Total configs** | **5** | **64** |\n",
    "\n",
    "The `cluster_selection_epsilon` parameter is particularly important — it merges HDBSCAN leaf clusters that are within `epsilon` distance of each other, directly controlling the granularity of the final partition. Larger `min_cluster_size` values also shift the sweep toward coarser clusterings that are more aligned with the 27 ground-truth intents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Selection Metric — DBCV Instead of ARI\n",
    "\n",
    "**Problem:** The baseline selects the best HDBSCAN configuration by maximising ARI (Adjusted Rand Index) against the ground-truth intent labels. This is **circular** — the unsupervised clustering is being tuned using the very labels it aims to discover. In a real-world scenario these labels would not be available.\n",
    "\n",
    "**Fix:** v2 selects by **DBCV** (Density-Based Clustering Validation), an internal validity metric available via `clusterer.relative_validity_`. DBCV measures how well the clustering captures the density structure of the data without reference to any external labels.\n",
    "\n",
    "```python\n",
    "# ── Baseline (cell-18) ─────────────────────────────────\n",
    "best_mcs = sweep_df.loc[sweep_df[\"ari_vs_intent\"].idxmax(), \"min_cluster_size\"]\n",
    "\n",
    "# ── v2 (cell-18) ──────────────────────────────────────\n",
    "feasible = sweep_df[sweep_df[\"n_clusters\"] <= 30]\n",
    "best_row = feasible.loc[feasible[\"dbcv\"].idxmax()]\n",
    "```\n",
    "\n",
    "ARI is still computed and displayed for **comparison**, but it no longer drives parameter selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cluster Count Constraint (<= 30)\n",
    "\n",
    "**Problem:** Optimising DBCV without constraints can favour configurations with many tiny, tightly-packed clusters. DBCV rewards high intra-cluster density and low inter-cluster density, which is maximised when each dense region gets its own cluster — even if that means hundreds of micro-clusters.\n",
    "\n",
    "**Fix:** v2 filters the sweep results to only configurations producing **<= 30 clusters** before selecting the best DBCV score. This ensures the result is practically useful (comparable to the 27 ground-truth intents) while still using an unsupervised metric within that feasible region.\n",
    "\n",
    "```python\n",
    "feasible = sweep_df[sweep_df[\"n_clusters\"] <= 30]\n",
    "if feasible.empty:\n",
    "    raise RuntimeError(\"No configurations produced <= 30 clusters — expand the sweep grid.\")\n",
    "best_row = feasible.loc[feasible[\"dbcv\"].idxmax()]\n",
    "```\n",
    "\n",
    "The threshold of 30 was chosen to provide a small margin above the 27 known intents, allowing for the possibility that HDBSCAN discovers legitimate sub-intents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. DBCV Computation Enabled\n",
    "\n",
    "**Problem:** HDBSCAN's `relative_validity_` attribute (DBCV score) requires the minimum spanning tree to be computed, which is disabled by default. The baseline does not set `gen_min_span_tree=True`, so accessing `relative_validity_` would raise an `AttributeError`.\n",
    "\n",
    "**Fix:** v2 adds `gen_min_span_tree=True` to every HDBSCAN constructor — both in the sweep loop and in the final clustering run.\n",
    "\n",
    "```python\n",
    "c = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=mcs,\n",
    "    min_samples=ms,\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\",\n",
    "    cluster_selection_epsilon=eps,\n",
    "    gen_min_span_tree=True   # <-- enables DBCV\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Full-Dataset Evaluation via Approximate Predict\n",
    "\n",
    "**Problem:** The baseline evaluates ARI and NMI only on non-noise points (those with `hdbscan_cluster != -1`). When 18.7% of the data is labelled as noise and excluded, the reported metrics are optimistic — the hardest-to-cluster points are simply dropped.\n",
    "\n",
    "**Fix:** v2 uses `hdbscan.approximate_predict()` to assign noise points to their nearest cluster, then reports metrics in **two modes**:\n",
    "\n",
    "| Scope | Description |\n",
    "|-------|-------------|\n",
    "| **Excl. noise** | Standard approach (matches baseline), evaluates only assigned points |\n",
    "| **Full dataset** | All 26,872 points evaluated after noise assignment via `approximate_predict` |\n",
    "\n",
    "```python\n",
    "# ── Baseline (cell-23) ─────────────────────────────────\n",
    "valid_mask = df[\"hdbscan_cluster\"] != -1\n",
    "ari_intent = adjusted_rand_score(df.loc[valid_mask, \"intent_label\"], pred)\n",
    "# (only non-noise points evaluated)\n",
    "\n",
    "# ── v2 (cell-23) ──────────────────────────────────────\n",
    "# Excl. noise (same as baseline)\n",
    "ari_intent = adjusted_rand_score(df.loc[valid_mask, \"intent_label\"], pred)\n",
    "\n",
    "# Full dataset: assign noise to nearest cluster\n",
    "labels_full = cluster_labels.copy()\n",
    "noise_idx = np.where(labels_full == -1)[0]\n",
    "if len(noise_idx) > 0:\n",
    "    approx_labels, _ = hdbscan.approximate_predict(clusterer, embeddings_umap_cluster[noise_idx])\n",
    "    labels_full[noise_idx] = approx_labels\n",
    "ari_intent_full = adjusted_rand_score(df[\"intent_label\"], labels_full)\n",
    "```\n",
    "\n",
    "This gives a more honest picture of how well the clustering covers the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cosine Distance for Representative Sampling\n",
    "\n",
    "**Problem:** When selecting representative utterances for LLM-based labeling, the baseline picks the samples closest to the cluster centroid using **Euclidean distance**. However, the entire pipeline (UMAP, sentence embeddings) operates in cosine space. Euclidean distance in high dimensions can be dominated by vector magnitude rather than direction.\n",
    "\n",
    "**Fix:** v2 uses **cosine distance** (1 - cosine similarity) for centroid-based sampling, which is consistent with the rest of the pipeline.\n",
    "\n",
    "```python\n",
    "# ── Baseline (cell-32) ─────────────────────────────────\n",
    "def get_representative_samples(df, cluster_id, embeddings, n=8):\n",
    "    ...\n",
    "    centroid = cluster_embs.mean(axis=0)\n",
    "    distances = np.linalg.norm(cluster_embs - centroid, axis=1)  # Euclidean\n",
    "    closest_idx = distances.argsort()[:n]\n",
    "    ...\n",
    "\n",
    "# ── v2 (cell-32) ──────────────────────────────────────\n",
    "def get_representative_samples(df, cluster_id, embeddings, n=8):\n",
    "    ...\n",
    "    centroid = cluster_embs.mean(axis=0)\n",
    "    norms_emb = np.linalg.norm(cluster_embs, axis=1)\n",
    "    norm_cent = np.linalg.norm(centroid)\n",
    "    cosine_dists = 1 - (cluster_embs @ centroid) / (norms_emb * norm_cent + 1e-10)  # cosine\n",
    "    closest_idx = cosine_dists.argsort()[:n]\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Concurrent LLM API Calls\n",
    "\n",
    "**Problem:** The baseline labels clusters by calling the Anthropic API sequentially in a for-loop. With 21 clusters, this means 21 serial HTTP round-trips.\n",
    "\n",
    "**Fix:** v2 uses `anthropic.AsyncAnthropic` with `asyncio.gather` to issue all LLM labeling requests concurrently.\n",
    "\n",
    "```python\n",
    "# ── Baseline (cell-32) ─────────────────────────────────\n",
    "client = anthropic.Anthropic()\n",
    "llm_labels = {}\n",
    "for cid in sorted(valid_df[\"hdbscan_cluster\"].unique()):\n",
    "    response = client.messages.create(...)  # sequential\n",
    "    llm_labels[cid] = response.content[0].text.strip()\n",
    "\n",
    "# ── v2 (cell-32) ──────────────────────────────────────\n",
    "async def label_cluster(async_client, cid, samples, tfidf_label):\n",
    "    response = await async_client.messages.create(...)\n",
    "    return cid, response.content[0].text.strip()\n",
    "\n",
    "async def label_all_clusters():\n",
    "    async_client = anthropic.AsyncAnthropic()\n",
    "    tasks = [label_cluster(async_client, cid, ...) for cid in cluster_ids]\n",
    "    results = await asyncio.gather(*tasks)  # concurrent\n",
    "    return dict(results)\n",
    "\n",
    "llm_labels = await label_all_clusters()\n",
    "```\n",
    "\n",
    "**Impact:** With 21 clusters, the total LLM labeling time drops from ~21x a single call to roughly 1-2x (bounded by the slowest response), a ~10-15x speedup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Baseline Results (from `clustering_analysis.ipynb`)\n",
    "\n",
    "| Setting | Value |\n",
    "|---------|-------|\n",
    "| Sweep type | 1D (`min_cluster_size` only, 5 configs) |\n",
    "| Selection metric | ARI vs intent (circular) |\n",
    "| Best `min_cluster_size` | 500 |\n",
    "| `min_samples` | 10 (fixed) |\n",
    "| `cluster_selection_epsilon` | 0.0 (not used) |\n",
    "| Clusters found | 21 |\n",
    "| Noise points | 5,022 (18.7%) |\n",
    "| DBCV | not computed |\n",
    "\n",
    "| Metric | Value | Scope |\n",
    "|--------|-------|-------|\n",
    "| ARI vs Intent (27) | 0.6961 | Excl. noise only |\n",
    "| NMI vs Intent (27) | 0.8751 | Excl. noise only |\n",
    "| ARI vs Category (11) | 0.6317 | Excl. noise only |\n",
    "| NMI vs Category (11) | 0.8368 | Excl. noise only |\n",
    "| Mean per-intent purity | 0.883 | Excl. noise only |\n",
    "| Min per-intent purity | 0.446 | Excl. noise only |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## How to Compare Results\n",
    "\n",
    "After running both notebooks end-to-end on the same dataset, compare:\n",
    "\n",
    "1. **Cluster count** — both should produce a manageable number of clusters (< 30)\n",
    "2. **Noise %** — v2 may have different noise rates depending on the DBCV-optimal parameters\n",
    "3. **ARI / NMI (excl. noise)** — direct comparison with the baseline's reported metrics\n",
    "4. **ARI / NMI (full dataset)** — v2-only; gives an honest evaluation including the hardest points\n",
    "5. **DBCV** — v2-only; internal validity measure (higher is better)\n",
    "6. **Per-intent purity** — how cleanly each ground-truth intent maps to a single cluster\n",
    "\n",
    "The v2 results should be compared with caution: since v2 selects parameters by DBCV rather than ARI, its ARI may be slightly lower than the baseline's (which was optimised directly for ARI). The more meaningful comparison is whether the clustering is *structurally sound* (high DBCV, reasonable cluster count) while still achieving competitive ARI/NMI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Contact Center (Python 3.13)",
   "language": "python",
   "name": "contact-center"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}