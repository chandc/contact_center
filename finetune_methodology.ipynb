{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Fine-Tuning Methodology\n",
    "\n",
    "This document describes the methodology used to fine-tune a sentence embedding model for customer support utterance clustering. The implementation is in [`finetune_embeddings.ipynb`](finetune_embeddings.ipynb).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Statement\n",
    "\n",
    "We use sentence embeddings to cluster 26,872 customer support utterances into groups that align with 27 ground-truth intents (e.g., `cancel_order`, `track_refund`, `change_shipping_address`). The clustering pipeline is:\n",
    "\n",
    "```\n",
    "Utterance text \u2192 Sentence Embedding (384d) \u2192 UMAP (15d) \u2192 HDBSCAN \u2192 Cluster labels\n",
    "```\n",
    "\n",
    "The pre-trained `all-MiniLM-L6-v2` model produces general-purpose embeddings. While it captures broad semantic similarity, it was not trained on customer support language or optimized to distinguish between the specific intents in our dataset. Fine-tuning adapts the embedding space so that **utterances with the same intent are pulled closer together** and **utterances with different intents are pushed apart**, directly improving downstream clustering quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset\n",
    "\n",
    "**Source:** [`bitext/Bitext-customer-support-llm-chatbot-training-dataset`](https://huggingface.co/datasets/bitext/Bitext-customer-support-llm-chatbot-training-dataset) from HuggingFace Datasets Hub.\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| Total samples | 26,872 |\n",
    "| Intents (fine-grained) | 27 classes |\n",
    "| Categories (coarse) | 11 classes |\n",
    "| Avg samples per intent | ~995 |\n",
    "| Text column used | `instruction` |\n",
    "\n",
    "Each row contains a customer support utterance (`instruction`) labeled with a ground-truth `intent` (e.g., `cancel_order`) and a broader `category` (e.g., `ORDER`). We use the `intent` labels as supervision for fine-tuning and both `intent` and `category` labels for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Base Model\n",
    "\n",
    "**Model:** [`all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| Architecture | MiniLM (6 layers, 384 hidden dim) |\n",
    "| Embedding dimensions | 384 |\n",
    "| Max sequence length | 256 tokens |\n",
    "| Parameters | ~22.7M |\n",
    "| Pre-training | Distilled from `all-MiniLM-L12-v2`, trained on 1B+ sentence pairs |\n",
    "\n",
    "This model was chosen because it is lightweight, fast to encode, and optimized for short texts\u2014well-suited for customer support utterances that are typically 5\u201325 words long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train / Validation Split\n",
    "\n",
    "We perform a **stratified 80/20 split at the utterance level**, not at the pair level.\n",
    "\n",
    "| Split | Utterances | Purpose |\n",
    "|-------|-----------|----------|\n",
    "| Train | ~21,498 | Generate training pairs |\n",
    "| Validation | ~5,374 | Generate evaluation triplets |\n",
    "\n",
    "### Why split at the utterance level?\n",
    "\n",
    "If we split at the pair level, the same utterance could appear in both a training pair and a validation pair. The model would memorize individual utterances rather than learning generalizable intent representations. By splitting utterances first and then generating pairs within each split, we guarantee **zero data leakage**.\n",
    "\n",
    "Stratification ensures every intent is proportionally represented in both splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Data: Pair Generation\n",
    "\n",
    "### Pair format\n",
    "\n",
    "Each training example is an **(anchor, positive)** pair where both utterances share the same intent:\n",
    "\n",
    "```\n",
    "Anchor:   \"I need assistance talking with an agent\"\n",
    "Positive: \"I want help to speak to a live person\"\n",
    "```\n",
    "\n",
    "Both belong to the `contact_human_agent` intent.\n",
    "\n",
    "### Sampling strategy\n",
    "\n",
    "For each of the 27 intents, we randomly sample up to **1,000 pairs** from all possible C(n, 2) combinations within the training split. This yields approximately **27,000 training pairs** total.\n",
    "\n",
    "| Detail | Value |\n",
    "|--------|-------|\n",
    "| Pairs per intent | up to 1,000 |\n",
    "| Total training pairs | ~27,000 |\n",
    "| Sampling method | Random without replacement |\n",
    "\n",
    "### Why 1,000 pairs per intent?\n",
    "\n",
    "- **Exhaustive pairing is impractical:** With ~800 utterances per intent in the training split, exhaustive C(800, 2) = 319,600 pairs per intent, or ~8.6M total. This is excessive for a small model.\n",
    "- **1,000 per intent balances signal and compute:** 27K pairs with batch_size=64 gives ~422 training steps per epoch. At 3 epochs, total training completes in minutes.\n",
    "- **Diminishing returns:** Contrastive learning with in-batch negatives is sample-efficient. Most of the learning happens in the first few thousand gradient updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validation Data: Triplet Generation\n",
    "\n",
    "For monitoring training progress, we generate **(anchor, positive, negative)** triplets from the validation split:\n",
    "\n",
    "```\n",
    "Anchor:   \"help me checking the available payment options\"   (check_payment_methods)\n",
    "Positive: \"I want to see what payment options are accepted\"  (check_payment_methods)\n",
    "Negative: \"I need help cancelling my purchase\"               (cancel_order)\n",
    "```\n",
    "\n",
    "| Detail | Value |\n",
    "|--------|-------|\n",
    "| Triplets per intent | up to 200 |\n",
    "| Total validation triplets | ~5,400 |\n",
    "| Evaluator | `TripletEvaluator` |\n",
    "\n",
    "The `TripletEvaluator` measures the percentage of triplets where `cosine(anchor, positive) > cosine(anchor, negative)`. A higher score means the model is better at placing same-intent utterances closer together than different-intent ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Loss Function: MultipleNegativesRankingLoss\n",
    "\n",
    "### What it does\n",
    "\n",
    "`MultipleNegativesRankingLoss` (MNRL) implements the **InfoNCE** contrastive objective. Given a batch of (anchor, positive) pairs, it:\n",
    "\n",
    "1. Encodes all anchors and all positives in the batch\n",
    "2. Computes the cosine similarity matrix between all anchors and all positives\n",
    "3. Treats each anchor's corresponding positive as the correct match\n",
    "4. All other positives in the batch serve as **in-batch negatives**\n",
    "5. Applies cross-entropy loss over the similarity scores\n",
    "\n",
    "### Illustration\n",
    "\n",
    "With a batch of 4 pairs, the similarity matrix looks like:\n",
    "\n",
    "```\n",
    "              Positive_0  Positive_1  Positive_2  Positive_3\n",
    "Anchor_0     [  0.92  ]    0.31        0.45        0.28       \u2190 label = 0\n",
    "Anchor_1       0.35     [  0.88  ]    0.40        0.33       \u2190 label = 1\n",
    "Anchor_2       0.41       0.37      [  0.90  ]    0.29       \u2190 label = 2\n",
    "Anchor_3       0.30       0.34        0.32      [  0.85  ]   \u2190 label = 3\n",
    "```\n",
    "\n",
    "The diagonal entries (bracketed) are the correct anchor-positive matches. The loss pushes diagonal scores up and off-diagonal scores down.\n",
    "\n",
    "### Why MNRL?\n",
    "\n",
    "- **Only requires positive pairs** \u2014 no need to explicitly mine hard negatives\n",
    "- **In-batch negatives are free** \u2014 a batch of 64 gives 63 negatives per anchor\n",
    "- **Scales with batch size** \u2014 larger batches provide more diverse negatives\n",
    "- **Standard choice** for sentence-transformers fine-tuning when labeled pairs are available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Hyperparameters\n",
    "\n",
    "| Parameter | Value | Rationale |\n",
    "|-----------|-------|-----------|\n",
    "| **Epochs** | 3 | Standard for fine-tuning small pre-trained models on moderate-sized data. Enough to converge without overfitting. |\n",
    "| **Batch size** | 64 | Larger batches provide more in-batch negatives per anchor (63 negatives), strengthening the contrastive signal. Limited by GPU/MPS memory. |\n",
    "| **Learning rate** | 2e-5 | Default for sentence-transformers fine-tuning. Low enough to preserve pre-trained knowledge while adapting to the domain. |\n",
    "| **Warmup** | 10% of total steps | Gradually ramps the learning rate to avoid large early parameter updates that could destabilize the pre-trained weights. |\n",
    "| **Scheduler** | WarmupLinear | Linear decay after warmup. Standard default for transformer fine-tuning. |\n",
    "| **Optimizer** | AdamW | Default optimizer in sentence-transformers. |\n",
    "| **Evaluation frequency** | Every ~211 steps (twice per epoch) | Frequent enough to monitor convergence and save the best checkpoint. |\n",
    "| **Save strategy** | Best model by TripletEvaluator score | Only the checkpoint with the highest triplet accuracy is saved, avoiding overfitting to later epochs. |\n",
    "\n",
    "### Training scale\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Steps per epoch | ~422 |\n",
    "| Total training steps | ~1,266 |\n",
    "| Warmup steps | ~127 |\n",
    "| Trainable parameters | ~22.7M |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation Methodology\n",
    "\n",
    "### Clustering pipeline (identical to baseline)\n",
    "\n",
    "To ensure a fair comparison, we re-run the **exact same** clustering pipeline from `clustering_analysis.ipynb` on both base and fine-tuned embeddings:\n",
    "\n",
    "| Stage | Parameters |\n",
    "|-------|------------|\n",
    "| **UMAP** (clustering) | 15 dims, n_neighbors=15, min_dist=0.0, cosine metric |\n",
    "| **HDBSCAN** | min_cluster_size=500, min_samples=10, EOM selection |\n",
    "| **UMAP** (visualization) | 2 dims, n_neighbors=15, min_dist=0.1, cosine metric |\n",
    "\n",
    "We intentionally do **not** re-tune HDBSCAN hyperparameters for the fine-tuned embeddings. Using fixed parameters isolates the effect of the embeddings from hyperparameter tuning.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "All metrics exclude HDBSCAN noise points (label = -1).\n",
    "\n",
    "| Metric | Range | What it measures |\n",
    "|--------|-------|------------------|\n",
    "| **ARI** (Adjusted Rand Index) | [-1, 1] | Agreement between predicted clusters and ground-truth labels, adjusted for chance. 1 = perfect, 0 = random. |\n",
    "| **NMI** (Normalized Mutual Information) | [0, 1] | Mutual information between cluster assignments and ground-truth, normalized. 1 = perfect correlation. |\n",
    "| **Per-intent purity** | [0, 1] | For each ground-truth intent, the fraction of its non-noise samples in the single most common cluster. Higher = cleaner clusters. |\n",
    "| **Embedding separation** | (-1, 1) | Difference between mean intra-class cosine similarity and mean inter-class cosine similarity. Higher = better class separation in the embedding space. |\n",
    "\n",
    "Metrics are computed against both **intent** (27 classes) and **category** (11 classes) ground-truth labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualizations Produced\n",
    "\n",
    "The fine-tuning notebook generates the following comparative visualizations:\n",
    "\n",
    "| Visualization | Purpose |\n",
    "|---------------|----------|\n",
    "| **Side-by-side UMAP** (intent-colored) | Shows whether intent clusters become tighter and more separated after fine-tuning |\n",
    "| **Side-by-side UMAP** (HDBSCAN-colored) | Compares discovered cluster structure and noise levels |\n",
    "| **ARI / NMI bar chart** | Direct numeric comparison of clustering quality |\n",
    "| **Per-intent purity bars** | Shows which intents improved or degraded, and by how much |\n",
    "| **Cosine similarity histograms** | Visualizes the shift in intra-class vs inter-class similarity distributions |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Design Decisions & Trade-offs\n",
    "\n",
    "### Why fine-tune rather than use a larger model?\n",
    "\n",
    "A larger pre-trained model (e.g., `all-mpnet-base-v2` at 768d) might improve clustering without fine-tuning, but:\n",
    "- Fine-tuning a smaller model is faster and cheaper to run at inference time\n",
    "- Domain-specific adaptation often outperforms generic larger models\n",
    "- 384d embeddings use less memory for UMAP/HDBSCAN on the full dataset\n",
    "\n",
    "### Why MNRL over triplet loss?\n",
    "\n",
    "- **Triplet loss** requires explicit (anchor, positive, negative) triplets and is sensitive to hard-negative mining strategy\n",
    "- **MNRL** only needs positive pairs and automatically leverages the entire batch as negatives\n",
    "- With batch_size=64, MNRL effectively sees 63 negatives per anchor\u2014more diverse than hand-selected triplets\n",
    "\n",
    "### Why not use the full C(n,2) pairs?\n",
    "\n",
    "Exhaustive pairing from ~800 samples per intent would produce ~320K pairs per intent (~8.6M total). This is:\n",
    "- Computationally wasteful for a 22.7M parameter model\n",
    "- Highly redundant (many near-duplicate pairs from paraphrases)\n",
    "- Unnecessary since MNRL is sample-efficient via in-batch negatives\n",
    "\n",
    "### Why fixed HDBSCAN parameters for evaluation?\n",
    "\n",
    "Re-tuning HDBSCAN for fine-tuned embeddings would conflate two effects: better embeddings vs. better hyperparameters. Using identical parameters (min_cluster_size=500) isolates the embedding improvement. In production, you would re-tune HDBSCAN to maximize the benefit of the new embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Reproducibility\n",
    "\n",
    "### Random seeds\n",
    "\n",
    "All stochastic operations are seeded with `SEED = 42`:\n",
    "- `random.seed(42)` \u2014 pair/triplet sampling\n",
    "- `np.random.seed(42)` \u2014 NumPy operations\n",
    "- `torch.manual_seed(42)` \u2014 model initialization and training\n",
    "- `random_state=42` \u2014 UMAP projections and train/val split\n",
    "\n",
    "### Environment\n",
    "\n",
    "| Component | Version / Detail |\n",
    "|-----------|------------------|\n",
    "| Python | 3.13 |\n",
    "| Package manager | `uv` |\n",
    "| sentence-transformers | (see `uv.lock`) |\n",
    "| PyTorch device | MPS (Apple Silicon), CUDA, or CPU (auto-detected) |\n",
    "| Dependencies | Locked in `pyproject.toml` + `uv.lock` |\n",
    "\n",
    "### To reproduce\n",
    "\n",
    "```bash\n",
    "# 1. Install dependencies\n",
    "uv sync\n",
    "\n",
    "# 2. Register the Jupyter kernel\n",
    "uv run python -m ipykernel install --user --name contact-center --display-name \"Contact Center (Python 3.13)\"\n",
    "\n",
    "# 3. Run the notebook end-to-end\n",
    "#    Select the \"Contact Center (Python 3.13)\" kernel and run all cells in finetune_embeddings.ipynb\n",
    "```\n",
    "\n",
    "The fine-tuned model is saved to `./finetuned-MiniLM-L6-v2-customer-support/` (excluded from git via `.gitignore`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Potential Improvements\n",
    "\n",
    "| Improvement | Description |\n",
    "|-------------|-------------|\n",
    "| **Hard-negative mining** | Use `GISTEmbedLoss` or mine hard negatives (high similarity, different intent) for stronger contrastive signal |\n",
    "| **Larger base model** | Try `all-mpnet-base-v2` (768d) or `BAAI/bge-base-en-v1.5` as starting points |\n",
    "| **More epochs with early stopping** | Monitor validation loss and stop when it plateaus |\n",
    "| **Augmented pairs** | Use paraphrase generation to create additional training pairs for low-frequency intents |\n",
    "| **Multi-task training** | Jointly optimize for intent and category separation using `MatryoshkaLoss` or multi-label supervision |\n",
    "| **Re-tune HDBSCAN** | After fine-tuning, sweep HDBSCAN parameters again to find the optimal clustering for the new embedding space |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Contact Center (Python 3.13)",
   "language": "python",
   "name": "contact-center"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}